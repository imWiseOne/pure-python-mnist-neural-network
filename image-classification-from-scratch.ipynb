{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "544772ac-14fc-46b8-9282-9132c9d4590b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import gzip\n",
    "import struct\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "40477770-282c-4400-877f-210d934cf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, num_of_units, activation=\"relu\"):\n",
    "        self._a_in = None \n",
    "        self.num_of_units = num_of_units\n",
    "        self.activation=activation\n",
    "        self.biases = [0.0 for _ in range(num_of_units)]\n",
    "        self.weights = None\n",
    "    @property\n",
    "    def a_in(self):\n",
    "        return self._a_in\n",
    "    @a_in.setter\n",
    "    def a_in(self, value):\n",
    "        self._a_in = value\n",
    "        if self.weights is None:\n",
    "            fan_in = len(self._a_in)\n",
    "            std = math.sqrt(2.0 / fan_in)  # He initialization\n",
    "            self.weights = [[random.gauss(0, std) for _ in range(fan_in)] \n",
    "                           for _ in range(self.num_of_units)]\n",
    "    def infer(self):\n",
    "        a_out = [0]*self.num_of_units\n",
    "        for i_unit in range(self.num_of_units):\n",
    "            z = self.biases[i_unit]\n",
    "            for i_feature in range(len(self._a_in)):\n",
    "                z += self._a_in[i_feature] * self.weights[i_unit][i_feature]\n",
    "            if self.activation==\"relu\":\n",
    "                a_out[i_unit] = 0 if z < 0 else z #ReLU\n",
    "            elif self.activation==\"linear\":\n",
    "                a_out[i_unit] = z\n",
    "        return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "97d1f0b0-bfd6-45d2-bf06-446228833df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassificationNet():\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.best_weights_found = []\n",
    "        self.best_biases_found = []\n",
    "        # Activation values cache for faster backprop\n",
    "        self.activation_values = None\n",
    "\n",
    "    def infer(self, initial_a_in):\n",
    "        activation_value = initial_a_in\n",
    "        self.activation_values = []\n",
    "        self.activation_values.append(initial_a_in)\n",
    "        for layer in self.layers:\n",
    "            layer.a_in = activation_value\n",
    "            activation_value = layer.infer()\n",
    "            self.activation_values.append(activation_value)\n",
    "        # Softmaxing\n",
    "        max_z = max(activation_value) # to avoid overflow\n",
    "        exponents = [math.exp(z - max_z) for z in activation_value]\n",
    "        exponents_sum = sum(exponents)\n",
    "        return [exp / exponents_sum for exp in exponents]\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        best_epoch_loss = 1\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i, x in enumerate(X):\n",
    "                y_hat = self.infer(x) # probabilities\n",
    "                correct_y_hat = sum([y_hat[j] * y[i][j] for j in range(len(y_hat))])\n",
    "                total_loss += -1 * math.log(correct_y_hat)\n",
    "    \n",
    "                # Initializing lists\n",
    "                d_weights = []\n",
    "                d_biases = []\n",
    "                for layer in self.layers:\n",
    "                    d_weights.append([[0]*len(layer.a_in) for _ in range(layer.num_of_units)])\n",
    "                    d_biases.append([0]*layer.num_of_units)\n",
    "                    \n",
    "                ## Backpropagaaatioooooon!\n",
    "                dL_dz = [y_hat[j] - y[i][j] for j in range(len(y_hat))]\n",
    "                da_carried = dL_dz\n",
    "                for rev_i, layer in enumerate(reversed(self.layers)):\n",
    "                    layer_i = len(self.layers) - 1 - rev_i\n",
    "    \n",
    "                    # ReLU's impact on derivatives ----TODO-----\n",
    "                    a_out = self.activation_values[layer_i+1]\n",
    "                    for neuron_i in range(layer.num_of_units):\n",
    "                        if layer.activation == \"relu\":\n",
    "                            da_carried[neuron_i] *= 1 if a_out[neuron_i] > 0 else 0\n",
    "                            \n",
    "                    # Partial derivatives of weights and biases\n",
    "                    for neuron_i in range(layer.num_of_units):\n",
    "                        d_biases[layer_i][neuron_i] = da_carried[neuron_i]\n",
    "                        for weight_i in range(len(layer.a_in)):\n",
    "                            d_weights[layer_i][neuron_i][weight_i] = (\n",
    "                                da_carried[neuron_i] * layer.a_in[weight_i]\n",
    "                            )\n",
    "                    # Partial derivatives of the previous layer's activations\n",
    "                    new_da_carried = [0]*len(layer.a_in)\n",
    "                    for prev_a_i in range(len(layer.a_in)):\n",
    "                        for neuron_i in range(layer.num_of_units):\n",
    "                            new_da_carried[prev_a_i] += da_carried[neuron_i] * layer.weights[neuron_i][prev_a_i]\n",
    "\n",
    "                    da_carried = new_da_carried\n",
    "    \n",
    "    \n",
    "                \n",
    "                # Gradient Descent\n",
    "                learning_rate = 0.005\n",
    "                for layer_i, layer in enumerate(self.layers):\n",
    "                    for neuron_i in range(layer.num_of_units):\n",
    "                        layer.biases[neuron_i] -= learning_rate * d_biases[layer_i][neuron_i]\n",
    "                        for weight_i in range(len(layer.a_in)):\n",
    "                            layer.weights[neuron_i][weight_i] -= learning_rate * d_weights[layer_i][neuron_i][weight_i]\n",
    "\n",
    "            # Printing epoch loss and saving best parameters\n",
    "            epoch_loss = total_loss/len(X)\n",
    "            print(f\"Epoch {epoch}. Loss: {epoch_loss}\")\n",
    "            if epoch_loss < best_epoch_loss:\n",
    "                self.best_weights_found = []\n",
    "                self.best_biases_found = []\n",
    "                for layer in self.layers:\n",
    "                    layer_weights = []\n",
    "                    for neuron_i in range(layer.num_of_units):\n",
    "                        neuron_weights = [layer.weights[neuron_i][weight_i] for weight_i in range(len(layer.a_in))]\n",
    "                        layer_weights.append(neuron_weights)\n",
    "                    self.best_weights_found.append(layer_weights)\n",
    "                    self.best_biases_found.append(layer.biases[:])  # Deep copy biases\n",
    "                best_epoch_loss = epoch_loss\n",
    "                save_best_weights(self, \"best_params.json\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        correct = 0\n",
    "        for x, y in zip(X_test, y_test):\n",
    "            y_hat = self.infer(x)\n",
    "            predicted = y_hat.index(max(y_hat))\n",
    "            true_label = y.index(max(y))\n",
    "            if predicted == true_label:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(X_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0320789b-12a4-43d5-b87f-8a3bf1ca7e7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST - Blatantly copied from Grok - no questions asked\n",
    "def load_mnist(subset_size=10000):\n",
    "    def read_images(filename):\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "            images = []\n",
    "            for _ in range(num):\n",
    "                pixels = [b / 255.0 for b in f.read(rows * cols)]  # Normalize to [0, 1]\n",
    "                images.append(pixels)\n",
    "        return images\n",
    "\n",
    "    def read_labels(filename):\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            magic, num = struct.unpack('>II', f.read(8))\n",
    "            labels = [b for b in f.read(num)]\n",
    "            # Convert to one-hot encoding\n",
    "            one_hot = [[1 if j == label else 0 for j in range(10)] for label in labels]\n",
    "        return one_hot\n",
    "\n",
    "    X_train = read_images('Documents/Datasets/MNIST/train-images-idx3-ubyte.gz')[:subset_size]\n",
    "    y_train = read_labels('Documents/Datasets/MNIST/train-labels-idx1-ubyte.gz')[:subset_size]\n",
    "    X_test = read_images('Documents/Datasets/MNIST/t10k-images-idx3-ubyte.gz')\n",
    "    y_test = read_labels('Documents/Datasets/MNIST/t10k-labels-idx1-ubyte.gz')\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e4a1ead0-fc29-4701-ae5c-cfc77246f220",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Predict image\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def predict_image(jpg_filename, neural_network, display_image=True):\n",
    "    img = Image.open(jpg_filename)\n",
    "    img = img.convert('L')  # Convert to grayscale\n",
    "    img = img.resize((28, 28))  # Resize to 28x28 if necessary\n",
    "    \n",
    "    pixels = [pixel / 255.0 for pixel in img.getdata()]\n",
    "    \n",
    "    prediction = neural_network.infer(pixels)\n",
    "    \n",
    "    predicted_digit = prediction.index(max(prediction))\n",
    "    \n",
    "    if display_image:\n",
    "        display(img)\n",
    "    \n",
    "    return predicted_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e021f6d-9c3c-4baf-aa0e-293c8b3142fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.5471447907119127\n",
      "Epoch 1. Loss: 0.2971334920255449\n",
      "Epoch 2. Loss: 0.23862759410115306\n",
      "Epoch 3. Loss: 0.20143824293538612\n",
      "Epoch 4. Loss: 0.1738633263816597\n",
      "Epoch 5. Loss: 0.1556997583326201\n"
     ]
    }
   ],
   "source": [
    "# Run experiment\n",
    "X_train, y_train, X_test, y_test = load_mnist(subset_size=10000)\n",
    "neural_network = MultiClassificationNet(Layer(16, \"relu\"), Layer(16, \"relu\"), Layer(10, \"linear\"))\n",
    "neural_network.train(X_train, y_train, epochs=100)\n",
    "accuracy = neural_network.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585a54b-6e25-45b3-b322-5cbf2a650fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_best_params(neural_network, filename):\n",
    "    model_data = {\n",
    "        'best_weights_found': neural_network.best_weights_found,\n",
    "        'best_biases_found': neural_network.best_biases_found\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model_data, f, indent=4)\n",
    "\n",
    "def restore_best_params(neural_network):\n",
    "        if neural_network.best_weights_found is None or neural_network.best_biases_found is None:\n",
    "            raise ValueError(\"No best parameters found.\")\n",
    "        for layer, weights, biases in zip(neural_network.layers, neural_network.best_weights_found, neural_network.best_biases_found):\n",
    "            layer.weights = copy.deepcopy(weights)\n",
    "            layer.biases = copy.deepcopy(biases)\n",
    "\n",
    "def load_best_params(neural_network, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        model_data = json.load(f)\n",
    "    neural_network.best_weights_found = model_data['best_weights_found']\n",
    "    neural_network.best_biases_found = model_data['best_biases_found']\n",
    "    # Restore weights and biases to the layers\n",
    "    restore_best_params(neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d6ca00ea-bc1d-4073-8134-ea2236f2cd48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+u+8K/DKTXPCt14q1fVl0fQoCyeebSSeRyCo3BFAymWILAnBUjHBIz/FHgqDRtFs9f0XXINb0O5mNt9qjiMLxTAE7HjYkrkAkeo5wAVzyNbHhXRh4g8W6TpDJO0d3dxxS+QMusZYb2HBxhcnJGBjJrZ+IXiRdW1g6NYR2kOg6NPNBpkVqAUEZIBbcPv7ygfJJyWPJqz4OaS7+H/jjTnG63W1gvFyu4xyJKACuTgZVmB4yR34weFrZ8J+Irjwn4p0/XbaMSSWkm4xk43oQVdc4OMqSM4OM5rsdd+GT6vGviD4fK2raDdOR5CH9/ZPuA8p1Y7jjcMHn5eTxhmjvrKX4d+BtT0bVotniDxEkDC3X71naxyMdzuMgmRlxsHQDLHPy15zRSqzI4dGKspyCDgg0MzO5d2LMxySTkk0lf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/UlEQVR4AY2SMQ6CQBBFB2LiBaQwsba0MjSeQhtP4AlMuAQF9zHegkobOxNPYNSE78wss7sIJC4Js/Pf52dYIBpYiWl+Y8JgHXeNkxAENMCZe2duH0nACsBFhJDTmrgAYqEGVylJ5FGGo8jO45k+3WpEJQ7iidaSh7HWdhMTLrTe2F4TdUanAKVHlCMPjU4atQUK7VK560sGiJrqAGXYT4C04EtW6l7nTtMIZpSpnEomaB4hogdfelhO9Qeg7csTPZ19h3YatgOnkPwLmd40Q4z+HM3Pwe5jsm2lonm1YfqcVVWcydwsTOU3eVtYr253PSkSun9HBP7Y6gh6+wIYI1w5Yms5zgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_best_weights(neural_network, \"best_params.json\")\n",
    "load_best_params(neural_network, \"best_params.json\")\n",
    "predict_image(\"Pictures/canvas.png\", neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e6272-4021-4f24-bb34-ef2715ad9710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
